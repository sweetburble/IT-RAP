
## Abstract
Adversarial perturbations that proactively defend against generative AI technologies, such as Deepfakes, often lose their effectiveness when subjected to common image transformations. 
This is because existing schemes focus on perturbations in the spatial domain (i.e., pixel) while image transformation often targets both the spatial and frequency domains. 
To overcome this, this paper presents Image Transformation-Robust Adversarial Perturbation (IT-RAP), a framework that learns a robust, multi-domain perturbation policy using Deep Reinforcement Learning (DRL).
ITRAP employs DRL to strategically allocate perturbations across a hybrid action space that includes both spatial and frequency domains.
This allows the agent to discover optimal strategies for perturbation and improve robustness against image transformations.
Our comprehensive experiments demonstrate that IT-RAP successfully disrupts deepfakes with an average success rate of 64.62% when targeting various image transformations.



<img width="350" alt="figure3" src="https://github.com/user-attachments/assets/1b57b727-ed88-4ed5-872d-9869fcef6efc" />
<img width="350" alt="figure3" src="https://github.com/user-attachments/assets/8a10db90-1266-4844-b450-3d2aba176749" />
<img width="500" alt="figure6" src="https://github.com/user-attachments/assets/d23d98a0-1112-4961-beb3-8a91d75070d0" />



## Datasets and Models 
<!-- 여기에 pre-trained 된 모델과 그 citation 또는 모델 링크 추가-->



## Attack Training & Attack Inference




## Results
<!-- 논문 2장에서 나온 내용 중 일부 핵심 내용을 가져와도 되고, Disrupting Deepfakes 또는 DF-RAP 논문만 언급하면 좋을 듯합니다. -->



## Related Works
<!-- 논문 2장에서 나온 내용 중 일부 핵심 내용을 가져와도 되고, Disrupting Deepfakes 또는 DF-RAP 논문만 언급하면 좋을 듯합니다. -->



## License 




## Acknowledges 



